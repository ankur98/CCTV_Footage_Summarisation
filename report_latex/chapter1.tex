\chapter{Introduction}

Introduction

\section{State of the art developments}

In this section, existing methods for summarizing videos and related information that helps achieve this task are discussed.

In \cite{rav2006making}, Yael Pritch and Alex Rav-Acha propose a method to effectively generate a synopsis of an endless video stream that can also be used as an index into the main video. An online phase includes tube detection in spatio-temporal domain, insertion of these tubes into an object queue, and removal upon reaching a space limit. The response phase then constructs a time-lapse video of the changing background, selection and stitching of tubes into a coherent video. Min-cut algorithm along with background subtraction has been used for extracting moving objects. Activity, collision and temporal consistency costs have been used a parameters for optimal tube arrangement.

In \cite{pritch2008nonchronological}, Shmuel Peleg and Yael Pritch, have presented a dynamic video synopsis technique where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times.

In \cite{rodriguez2010cram}, (CRAM: Compact Representation of Action in Movies), Mikel Rodriguez generates a compact video representation of a long sequence, which while preserving the general dynamics of the video features only the essential components. From the given input video, optical flows are generated. These are then represented as vectors in Clifford Fourier domain. Dynamic regions of flow are then identified within the phase spectrum volume. The likelihood of activities of relevance are then computed by correlating it with spatio-temporal maximum average correlation height filters. The final summary is then generated by a temporal-shift optimization. Although this method could detect specific actions, it couldn’t keep all the events in the final summary.

Sarit Ratovitch, Avishai Hendel and Shmuel Peleg, in their paper titled Clustered Synopsis of Surveillance Video \cite{pritch2009clustered}, present a different approach to generating video summaries, based on clustering of similar activities. Objects with similar activities are easy to watch simultaneously, which also makes spotting of outliers easier. This method is also suitable for creation of ground truth data. This paper covered three main topics, the definition of distance between activities, clustering of similar activities and efficient presentation of video summaries using obtained clusters.

In \cite{porter2003shortest}, (A Shortest Path Representation for Video Summarization), a new approach for video summarization is presented to select multiple key frames within an isolated video shot where there is camera motion, causing significant scene change. This can be done by determining dominant motion between frame pairs whose similarities are represented using a directed graph. A* algorithm is used to detect the shortest path and designate key frames. The overall set of key frames depict the essential video content and camera motions.

\cite{zivkovic2004improved} presents a very successful and highly used method for adaptive pixel-level background subtraction. Each pixel has probability density function separately. A pixel is considered to be part of the background if its new value if well described by its density function. This paper was an improvement on previous models which used Guassian mixture models with efficient update equations.

In \cite{redmon2016you}, an extremely fast object detection model, the YOLO model is described. While prior object detectors used classifiers to detect, this paper proposes object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network is used to predict both bounding boxes and its class probability, making end-to-end optimization easy. Although YOLO makes more localization errors, it is less likely to predict false positives compared to other object detectors like SSD, RCNN and Faster RCNN.

\section{Motivation}

Our prime motivation was the fact that manual analysis of CCTV footage is time consuming and laborious. With the increase in the number of CCTV’s being used, it is necessary to have a smart method to generate a summary of footage which highlights what is necessary and removes the rest. The idea of this system helping the users save tons of time and manual labour was what made us chose this topic. This project also helped us to explore deeper into the domain of computer vision, towards which we share a common interest.

\section{Problem Statement}

Manual analysis of CCTV footage is extremely time consuming and laborious. Thus there is a need for smart tools to generate useful insights and summaries.
The task at hand is to develop an efficient method to generate a summary from a given input CCTV footage. It must be able to preserve important content while eliminating unnecessary details, thus saving hours of manual work hours spent going through the entire video. In addition, there is a lack of a smart query-based system for summarization. The system must generate a summary based on user query.


\section{Objective}

The objectives of the system are set with the requirements of each module. The first objective is to develop a video summarization technique, that will take efficiently handle overlapping of events in the video. The second objective is to add the functionality of generating a summary based on user query. The last objective is to build a suitable GUI and integrate with the available back-end.

\section{Scope}

The main area of application of our project is for security purposes. It can be used for detection of crimes, detecting suspicious activities etc. Hence it will be of use to the security forces and the police.

\section{Methodology}

The project has been split into 2 phases – the real-time phase and query phase.

Real-time phase includes all those processes that work on the input data from the CCTV before storing the intermediate result in the database. These include background masking, to separate the foreground and background, motion detection to capture frames of interest, tube extraction to store these frames of interest, object detection to gather metadata about the events in the video. This phase also includes generating a static background of the input video. The generated data is then stored in the database.

The next phase, the query phase, then takes in user query, selects the tubes based on the query, rearranges the tubes using Simulated Annealing algorithm, and then blends these rearranged tubes into the background using Poisson Blending to generate the video summary.


\section{Organization of Report}

This section gives the overall picture of the many chapters in this report.

Chapter 2 gives an overview of the project domain which describes the details of the software and techniques used to carried out the project.

Chapter 3 is on Software Requirement Specification which describes the assumptions and dependencies, user characteristics, functional requirements and constraints of the project.

Chapter 4 is High Level Design which elucidates the design phase in Software Development Life Cycle. This chapter talks about the design considerations like architectural strategies, general constraints, development methods and. It explains the project System Architecture and Data Flow Diagrams.

Chapter 5 is Detailed Design which explicates the two project modules. The functionality of each module represented as a flowchart is explained in this section.

Chapter 6 is Implementation which describes the technology used in the system. This section also explains programming language, development environment, code conventions followed.

Chapter 7 is on Software Testing which elaborates the test environment and briefly explains the test cases which were tried out during unit, integration and system testing.

Chapter 8 is Experimental Results which mentions the results found by the experimental analysis on different data sets. It talks about the inference made from the results.

Chapter 9 is Conclusion conveying the summary, limitations and future enhancements of the project.


\section{Summary}

This chapter deals with the introduction to the topic. The existing systems for video summarization are discussed. Research work, study material and papers on accessory software is also discussed. It also discusses in detail about the motivation, problem statement and the objectives of the project.